---
title: "An Exploration of Power calculations"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: united
    highlight: tango
    code_folding: hide
params:
  author: "Jake Bowers and Ben Jaques-Leslie"
  project_number: 0000
  project_name: "Statistical Standard Operating Procedures: Power Analysis"
  data_folder: "NA"
  sample_min: 100
  sample_max: 2000
  ate: -1
  ate_min: -10
  ate_max: 0 
  ate_increment: 1
  error_variance: .45
  treatment_probability: .5
  panels_min: 1
  panels_max: 4
  treatment_at_min: 1
  treatment_at_max: 4
  simulations: 100
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(skimr)
library(DataExplorer)
library(readr)
# library(oesrrr)
# library(oescolorrrs)
library(ggthemes)
library(DesignLibrary)
library(DeclareDesign)
library(coin)
library(MASS)
library(flextable)
## These next to make the sims go faster
library(future)
library(future.apply)
library(furrr)
```

```{r parse_params}
## In case you are not using RStudio but need the params
if (!exists("params")) {
  params <- rmarkdown::yaml_front_matter(here::here("analysis/scripts", "2233-power-calculations.Rmd"))$params
}
```

# Power calculations {.tabset .tabset-pills}

**Project number**: `r params$project_number`

**Project name**: `r params$project_name`

**Author**: `r params$author`

**Data**: `r params$data`'

This document presents two approaches to a simple simulation-based power
analysis. The design is relatively simple: 2 arms. And the outcomes are skewed
counts. The presumed pattern of treatment effects is also very skewed.

## Assumptions

The assumed average treatment effect is between `r params$ate_min` and `r
params$ate_max`. We are assuming that the use of USWDS will decrease the number
of issues by between `r params$ate_min` and `r params$ate_max`.

Based on the evaluation of the outcome indicators above, we are assuming that
the outcome indicators follow a negative binomial distribution --- and therefore
are bounded from below by 0.

We simulate individual website treatment effect using a negative binomial
distribution on the left side of zero as the treatment is unlikely to increase
issues and we expect large effects on some site but mostly small and/or no
effects.

We **define** the average treatment effect as

$$
\text{ate} = 1/n \sum_{i=1}^n Y_{i,1} - Y_{i,0}
$$

We **estimate** the average treatment effect using a difference of means
calculated using OLS with the following specification where $Y$ is the observed
value of the outcome such that $Y_i = Treatment_i Y_{i,1} + (1-Treatment_i)
Y_{i,0}$:

$$
Y_i = \beta_0 + \beta_1Treatment_i
$$

$Y$ is calculated by the number of issues or by the rank of the agency in number of issues.

## Functions

The function below calculates generates data (including baseline outcomes and
treatment randomization), estimates average treatment effects, and tests the
null hypothesis of no average effects, based on the assumptions above.


# DeclareDesign

We first try it using the Declare Design approach.

Power analysis looks at the operating characterists of hypothesis tests ---
"power" is the probability that a test rejects a hypothesis when it is false.
Mostly we are looking at rejecting the hypothesis of no effects when in fact
thare some effects.

Below we compare tests using three different test statistics that might have
different power to reject the null of no effects when the true effects are
mostly zero but some are large and the outcome distribution is skewed too.

We are using Normal approximations to the randomization distribution of the
test statistics below but assess the false positive rates of the tests as a
check on this assumption.

We will compare a test statistic using a difference of means (using OLS as our
difference-of-means calculator and using Neyman/HC2 standard errors), a
difference of mean ranks (again using OLS and Neyman/HC2 standard errors but
just on rank transformed outcomes). And we add a Wilcoxon-Mann-Whitney test
which also uses rank transformations.

```{r}
## Define a hypothesis test in a way that declaredesign likes: data in and data
## out
our_wctest <- function(data) {
  res <- coin::wilcox_test(
    Y ~ factor(Z),
    data = data,
    distribution = "asymptotic"
  )
  data.frame(p.value = pvalue(res)[[1]])
}
```

Setup a "designer" function that can take different parameters to create
baseline data, treatment effects, and tests (some of the tests are calculated
using "estimators" and we use this fact to check on the performance of power
analysis by looking at the bias of one of the estimators --- the difference of
means estimator should be an unbiased estimator of the overall average treatment
effect).

```{r}

a_designer <- function(in_population_size, in_ate, in_population_mean) {
  des <- declare_model(
    N = in_population_size,
    outcome_0 = rnbinom(in_population_size, size = 1, mu = in_population_mean),
    effect = -rnbinom(in_population_size, size = 1, mu = -in_ate)
  ) +
    declare_assignment(Z = randomizr::complete_ra(in_population_size, m = floor(in_population_size / 2))) +
    declare_potential_outcomes(Y ~ dplyr::case_when(
      effect * Z + outcome_0 < 0 ~ 0,
      effect * Z + outcome_0 >= 0 ~ effect * Z + outcome_0
    )) +
    declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +
    declare_measurement(Y = fabricatr::reveal_outcomes(Y ~ Z)) +
    declare_estimator(Y ~ Z, .method = estimatr::lm_robust, inquiry = "ATE", label = "OLS") +
    declare_estimator(I(rank(Y)) ~ Z, .method = estimatr::lm_robust, inquiry = "ATE", label = "OLSRANK") +
    declare_test(handler = label_test(our_wctest), label = "wilcox-test")

  return(des)
}
```

Now parameterize the designs:

```{r}
some_designs <- expand_design(
  a_designer,
  in_population_size = 100,
  in_ate = seq(params$ate_min, params$ate_max, params$ate_increment),
  in_population_mean = 5
)

## This next just tests the results of parameterizing the design, using the
## first parameterization
dat_1 <- draw_data(some_designs[["design_1"]])
head(dat_1)
table(dat_1$Y)
draw_estimates(some_designs[["design_1"]])
draw_estimands(some_designs[["design_1"]])
```

Now simulate many randomizations for each design. This next takes a while to
run.

```{r do_dd_sims, cache=TRUE}
## Only repeat the random assignment, not the creation of the baseline
## population for the simulation: the websites themselves are fixed.
steps_to_sim <- rep(1, length = length(names(some_designs[[1]])))
names(steps_to_sim) <- names(some_designs[[1]])
steps_to_sim["assignment"] <- 1000

plan(multisession, workers = availableCores() - 1)

## If we want the raw simulations:
## sims <- simulate_designs(designs,sims=steps_to_sim)
## But diagnose_designs also produces an object with all the sims so we can access that if we want anyway with
## some_diagnoses$simulations_df

some_diagnoses <- diagnose_designs(some_designs, sims = steps_to_sim, bootstrap_sims = FALSE)

plan(sequential)

## Saving the object. Perhaps add code above to skip this chunk if that file is recent and already exists and instead to just load it.
save(some_diagnoses, file = here::here("analysis/scripts/some_diagnoses.rda"))
```

We see a lot of information below (using the default diagnosands). 

First general checks on the procedure:

**Bias** If everything is working well we expect that the bias for the mean
difference estimator (labeled "OLS" below) should be close to zero: which is
what we see (variation around the bias should be of the order $2
\sqrt{1/\text{number of sims}}). Bias is not relevant for the testing approach
(the wilcoxon rank test --- it is not estimating anything, but is just testing
the null hypothesis of no effects using a rank-based test statistic). And bias
is not relevant for the estimator that is calculating the difference in mean
ranks --- we are only using that estimator for its side-effect of providing a
p-value for the null hypothesis of no average difference in rank.

```{r}
## load(file=here::here("analysis/scripts/some_diagnoses.rda"))
diag_dat <- some_diagnoses$diagnosands_df
glimpse(diag_dat)
diag_dat %>%
  filter(estimator == "OLS") %>%
  dplyr::select(in_ate, inquiry, estimator, outcome, mean_estimand, mean_estimate, bias, sd_estimate, rmse, power, coverage)
```

**Coverage / False Positive Rate** If everything is working well, we would
expect that the true estimand should be in the 95% confidence interval *at
least* 95% of the simulations. The false positive rate of the hypothesis test
here is 1-coverage and it should be no larger than .05 if we set $\alpha = .05$
for our rejection level. Notice that power is about .05 when the true estimand
is 0 --- this is another sign that the hypothesis test executed by `lm_robust`
is correctly controlling the false positive rate.

Let's compare the approaches: We see a bit more power using either of the
rank-based approaches than using the simple untransformed mean based approach
("OLS"). All approaches have controlled false positive rates (when the
hypothesis of no effects is true, then the tests reject rarely as they should
--- around 5% of the time at $\alpha=.05$) (Not showing coverage here because,
again, the estimand is only relevant for the OLS based approach.)

```{r}
diag_dat %>% dplyr::select(in_ate, inquiry, estimator, outcome, mean_estimand, power)
```

Once we have satisfied ourselves that the tests work well in the first place, we
can ask about power of the tests.

```{r}

diag_dat %>%
  ggplot(aes(x = in_ate, y = power, color = estimator)) +
  geom_line() +
  geom_hline(yintercept = .8) +
  ggthemes::theme_fivethirtyeight() +
  scale_x_continuous(
    limits = c(params$ate_min, params$ate_max),
    breaks = seq(params$ate_min, params$ate_max, params$ate_increment)
  ) +
  labs(
    title = "Power at different effects levels"
  ) +
  xlab("Average treatment effect") +
  ylab("Power") +
  theme(
    axis.title = element_text(),
    legend.title = element_blank(),
    legend.position = "right",
    legend.direction = "vertical",
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(
      hjust = 0.5,
      size = 10
    )
  )
```

```{r}
tbl_dd <- diag_dat %>%
  filter(power >= .8) %>%
  group_by(estimator) %>%
  filter(power == min(power)) %>%
  ungroup() %>%
  dplyr::select(estimator, power, in_ate) %>%
  mutate(estimator = str_to_title(estimator)) %>%
  rename_all(~ str_to_title(.)) %>%
  rename("Minimum detectable effect" = In_ate)

tbl_dd
```

```{r}
tbl_dd %>%
  flextable() %>%
  autofit()
```

## By Hand

Then we do it by hand to ensure that we understand what is going on above.

### Define Functions

```{r}
# First make the baseline data that will not vary across simulations.
make_baseline_population <- function(in_population_size,
                                     in_ate,
                                     in_population_mean) {
  # Simulated data
  pop_01 <- fabricate(
    N = in_population_size,
    effect = -rnbinom(in_population_size, size = 1, mu = -in_ate),
    outcome_0 =  rnbinom(in_population_size, size = 1, mu = in_population_mean)
  )
  return(pop_01)
}

make_new_experiment <- function(in_population_size, prop_treatment) {
  Z <- complete_ra(N = in_population_size, m = floor(in_population_size * prop_treatment))
  return(Z)
}

create_outcome_1 <- function(outcome_0, effect) {
  outcome_1 <- case_when(
    effect  + outcome_0 < 0 ~ 0,
    effect  + outcome_0 >= 0 ~ effect  + outcome_0
  )
  return(outcome_1)
}

reveal_observed_outcome <- function(outcome_0, outcome_1, Z) {
  outcome_observed <- Z * outcome_1 + (1 - Z) * outcome_0
  return(outcome_observed)
}
```

Now we want to combine the functions and want to be able to repeat the
experiment (re-randomize) on a given set of fixed baseline or population data.
In the motivating case, we have a fixed set of websites targetted by the
intervention.

This next function takes some parameters that we want to vary and returns a
function that can be repeated where the baseline data doesn't change but the
assignment of treatment is repeated.

```{r}
make_design_function <- function(in_population_size, prop_treatment, in_ate,
                                 in_population_mean) {
  pop <- make_baseline_population(in_population_size, in_ate, in_population_mean)
  function(thepop = pop) {
    ## The only thing that should change is Z. pop should be fixed.
    thepop <- thepop %>% mutate(
      Z = make_new_experiment(in_population_size, prop_treatment),
      outcome_1 = create_outcome_1(outcome_0, effect),
      ## Trying to figure out where the bias is coming from
      ## outcome_obs = reveal_observed_outcome(outcome_0, outcome_1, Z),
      outcome_obs = Z * outcome_1 + (1-Z) * outcome_0,
      true_effect = outcome_1 - outcome_0
    )
    true_ate <- mean(thepop$true_effect)
    est1 <- tidy(lm_robust(outcome_obs ~ Z, data = thepop))
    est2 <- tidy(lm_robust(I(rank(outcome_obs)) ~ Z, data = thepop))
    test1 <- pvalue(coin::wilcox_test(outcome_obs ~ factor(Z), data = thepop))[[1]]
    est1_res <- est1 %>%
      filter(term == "Z") %>%
      dplyr::select(estimate, std.error, p.value, conf.low, conf.high)
    est2_res <- est2 %>%
      filter(term == "Z") %>%
      dplyr::select(estimate, std.error, p.value, conf.low, conf.high)
    test1_res <- data.frame(estimate = NA, std.error = NA, p.value = test1, conf.low = NA, conf.high = NA)
    res <- rbind(est1_res, est2_res, test1_res)
    res$estimator <- c("diff_means", "diff_mean_ranks", "wc_test")
    res$estimand <- rep(true_ate, 3)
    return(res)
  }
}

## Test the function.
blah_fn <- make_design_function(
  in_population_size = 100,
  in_ate = -2,
  in_population_mean = 5,
  prop_treatment = .5
)
blah_fn()
```

This next is a function to make a new design function (that includes within it
the new data representing the parameters chosen).

```{r}
sim_designs <- function(in_population_size,
                        in_ate,
                        in_population_mean,
                        prop_treatment, nsims) {
  the_design_fn <- make_design_function(
    in_population_size = in_population_size,
    in_ate = in_ate,
    in_population_mean = in_population_mean,
    prop_treatment = prop_treatment
  )
  res <- map_dfr(1:nsims, ~ the_design_fn())
  res$in_ate <- in_ate
  res$in_population_size <- in_population_size
  res$prop_treatment <- prop_treatment
  res$nsims <- nsims
  return(res)
}

## Test the function
blah2 <- sim_designs(
  in_population_size = 100,
  in_ate = -2,
  in_population_mean = 5,
  prop_treatment = .5, nsims = 1000
)
head(blah2)

calc_operating_characteristics <- function(sim_obj) {
  out <- sim_obj %>%
    group_by(estimator, in_ate) %>%
    summarise(
      mean_ate = mean(estimand),
      beta_mean = mean(estimate),
      bias = mean(estimate - estimand),
      beta_sd = sd(estimate),
      mean_est_se = mean(std.error),
      power = mean(p.value <= 0.05),
      coverage = mean(estimand >= conf.low & estimand <= conf.high), .groups = "drop"
    ) %>%
    ungroup()
  return(out)
}

blah3 <- calc_operating_characteristics(blah2)
blah3 %>% filter(estimator == "diff_means")
```

Now do the full simulation

```{r}

plan(multisession, workers = availableCores() - 1)
params$ate_max <- 0

powers_new <- seq(params$ate_min, params$ate_max, params$ate_increment) %>%
  future_map_dfr(
    ~ sim_designs(
      in_population_size = 100,
      in_ate = .,
      in_population_mean = 5,
      prop_treatment = .5, nsims = 1000
    ),
    .options = furrr_options(seed = 123), .progress = TRUE
  )
plan(sequential)
```


```{r}
results <- calc_operating_characteristics(powers_new)
head(results)
```

Check the results again:

We should have low bias and coverage above 95% for the difference of means estimator and test statistic:
```{r}

max_sim_error <- 2*sqrt( .5 * (1-.5)/1000)

res_diff_means <- results %>% filter(estimator=="diff_means")

## Stop we don't see nominal coverage
stopifnot(all(abs(res_diff_means$coverage - .95) <= max_sim_error))

## Look at bias:
## Seems low compared to magnitude of the estimands and estimates.
summary(res_diff_means$bias)

```

Check false positive rate of the other tests (should be less than .05+/- simulation error):

```{r}

res_true_0 <- results %>% filter(in_ate==0)
res_true_0
stopifnot(all(abs(res_true_0$power - .05)<= max_sim_error))

```

The chart below shows the power for different average treatment effects. The wilcox test power is the same as the difference in mean ranks test power.

```{r}

results %>% 
  ggplot(aes(x = in_ate, y = power, color = estimator)) +
  geom_line() +
  geom_hline(yintercept = .8) +
  ggthemes::theme_fivethirtyeight() +
  scale_x_continuous(
    limits = c(params$ate_min, params$ate_max),
    breaks = seq(params$ate_min, params$ate_max, params$ate_increment)
  ) +
  labs(
    title = "Power at different effects levels" # ,
    # subtitle = glue::glue("Population size: {scales::comma(population_size_color_contrast_issues)}
    #    Population outcome mean: {scales::comma(population_mean_color_contrast_issues, accuracy = .01)}
    #    Population treatment proportion: {scales::comma(population_treatement_proportion, accuracy = .01)}
    #    Number of simulations: {scales::comma(params$simulations)}
    #    ")
  ) +
  xlab("Average treatment effect") +
  ylab("Power") +
  theme(
    axis.title = element_text(),
    legend.title = element_blank(),
    legend.position = "right",
    legend.direction = "vertical",
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(
      hjust = 0.5,
      size = 10
    )
  )
```

```{r}
tbl_mde_hand <- results %>%
  filter(power >= .8) %>%
  group_by(estimator) %>%
  filter(power == min(power)) %>%
  ungroup() %>%
  dplyr::select(estimator, power, in_ate) %>%
  mutate(estimator= str_to_title(estimator)) %>%
  rename_all(~ str_to_title(.)) %>%
  rename("Minimum detectable effect" = In_ate)

tbl_mde_hand
```

```{r}
tbl_mde_hand %>%
  flextable() %>%
  autofit()
```
